{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unlike-tanzania",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "guilty-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import umap\n",
    "# import umap.plot \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(os.path.expanduser('~/Research/Github/'),'PyTorch-VAE'))\n",
    "from datasets import WCDataset, WCShotgunDataset, WC3dDataset, WCRNNDataset\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'font.size':         24,\n",
    "                     'axes.linewidth':    3,\n",
    "                     'xtick.major.size':  5,\n",
    "                     'xtick.major.width': 2,\n",
    "                     'ytick.major.size':  5,\n",
    "                     'ytick.major.width': 2,\n",
    "                     'axes.spines.right': False,\n",
    "                     'axes.spines.top':   False,\n",
    "                     'font.sans-serif':  \"Arial\",\n",
    "                     'font.family':      \"sans-serif\",\n",
    "                    })\n",
    "\n",
    "########## Checks if path exists, if not then creates directory ##########\n",
    "def check_path(basepath, path):\n",
    "    if path in basepath:\n",
    "        return basepath\n",
    "    elif not os.path.exists(os.path.join(basepath, path)):\n",
    "        os.makedirs(os.path.join(basepath, path))\n",
    "        print('Added Directory:'+ os.path.join(basepath, path))\n",
    "        return os.path.join(basepath, path)\n",
    "    else:\n",
    "        return os.path.join(basepath, path)\n",
    "\n",
    "rootdir = os.path.expanduser('~/Research/FMEphys/')\n",
    "\n",
    "# Set up partial functions for directory managing\n",
    "join = partial(os.path.join,rootdir)\n",
    "checkDir = partial(check_path,rootdir)\n",
    "FigurePath = checkDir('Figures')\n",
    "\n",
    "savefigs=False\n",
    "\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "straight-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConvLSTM import ConvLSTM\n",
    "from experiment import VAEXperiment\n",
    "import yaml\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "instructional-deviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seuss/Research/FMEphys/logs2/VAE3dmp/version_7/checkpoints/epoch=23-step=82521.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/seuss/Research/FMEphys/Figures/version_7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = -1\n",
    "version = 7\n",
    "modeltype = '3dmp' # '3d'\n",
    "if modeltype=='shotgun': \n",
    "    filename =  os.path.join(os.path.expanduser('~/Research/Github/'),'PyTorch-VAE','configs/WC_vae_shotgun.yaml')\n",
    "    ckpt_path = glob.glob(os.path.expanduser('~/Research/FMEphys/logs2/VanillaVAE/version_3/checkpoints/*.ckpt'))[n]\n",
    "elif modeltype=='vanilla':\n",
    "    filename =  os.path.join(os.path.expanduser('~/Research/Github/'),'PyTorch-VAE','configs/WC_vae.yaml')\n",
    "    ckpt_path = glob.glob(os.path.expanduser('~/Research/FMEphys/logs2/VanillaVAE/version_0/checkpoints/*.ckpt'))[n]\n",
    "elif modeltype=='3d':\n",
    "    filename =  os.path.join(os.path.expanduser('~/Research/Github/'),'PyTorch-VAE','configs/WC_vae3d.yaml')\n",
    "    ckpt_path = glob.glob(os.path.expanduser('~/Research/FMEphys/logs2/VAE3d/version_4/checkpoints/*.ckpt'))[n]\n",
    "elif modeltype=='3dmp':\n",
    "    filename =  os.path.join(os.path.expanduser('~/Research/FMEphys/logs2/VAE3dmp/version_{:d}/WC_vae3dmp.yaml'.format(version)))\n",
    "    ckpt_path = glob.glob(os.path.expanduser('~/Research/FMEphys/logs2/VAE3dmp/version_{:d}/checkpoints/*.ckpt'.format(version)))[n]\n",
    "else:\n",
    "    raise ValueError(f'{value} is not a valid model type')\n",
    "print(ckpt_path)\n",
    "Epoch = int(os.path.basename(ckpt_path).split('=')[1].split('-')[0])\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "if modeltype=='shotgun': \n",
    "    config['exp_params']['data_path'] = os.path.expanduser('~/Research/FMEphys/')\n",
    "    config['exp_params']['csv_path_train'] = os.path.expanduser('~/Research/FMEphys//WCShotgun_Train_Data.csv')\n",
    "    config['exp_params']['csv_path_val'] = os.path.expanduser('~/Research/FMEphys//WCShotgun_Val_Data.csv')\n",
    "    config['logging_params']['save_dir'] = os.path.expanduser('~/Research/FMEphys/logs2/')\n",
    "elif modeltype=='vanilla':\n",
    "    config['exp_params']['data_path'] = os.path.expanduser('~/Research/FMEphys/')\n",
    "    config['exp_params']['csv_path_train'] = os.path.expanduser('~/Research/FMEphys//WC_Train_Data.csv')\n",
    "    config['exp_params']['csv_path_val'] = os.path.expanduser('~/Research/FMEphys//WC_Val_Data.csv')\n",
    "    config['logging_params']['save_dir'] = os.path.expanduser('~/Research/FMEphys/logs2/')\n",
    "elif (modeltype=='3d') | (modeltype=='3dmp'):\n",
    "    config['exp_params']['data_path'] = os.path.expanduser('~/Research/FMEphys/data')\n",
    "    config['exp_params']['csv_path_train'] = os.path.expanduser('~/Research/FMEphys/WC3d_Train_Data_SingVid.csv')\n",
    "    config['exp_params']['csv_path_val'] = os.path.expanduser('~/Research/FMEphys/WC3d_Val_Data_SingVid.csv')\n",
    "    config['logging_params']['save_dir'] = os.path.expanduser('~/Research/FMEphys/logs2/')\n",
    "config\n",
    "check_path(FigurePath,'version_{:d}'.format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boolean-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "SetRange = transforms.Lambda(lambda X: 2 * X - 1.)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                    transforms.Grayscale(num_output_channels=1),\n",
    "                    # transforms.RandomHorizontalFlip(),\n",
    "                    transforms.Resize((config['exp_params']['imgH_size'],config['exp_params']['imgW_size'])),\n",
    "                    transforms.ToTensor(),\n",
    "                    SetRange])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "experimental-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WCRNNDataset(root_dir = config['exp_params']['data_path'],\n",
    "                                csv_file = config['exp_params']['csv_path_train'],\n",
    "                                N_fm=config['exp_params']['N_fm'],\n",
    "                                transform=transform)\n",
    "    \n",
    "StartInd = 0\n",
    "config['exp_params']['batch_size'] = 30\n",
    "config['model_params']['tstrides'] = [1,1,1,1]\n",
    "NumBatches= 100 #len(dataset)\n",
    "\n",
    "train_dataset = Subset(dataset,torch.arange(StartInd,StartInd+config['exp_params']['batch_size']*NumBatches)) # 107162\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size= config['exp_params']['batch_size'],\n",
    "                              shuffle = False,\n",
    "                              drop_last=False,\n",
    "                              num_workers=7,\n",
    "                              pin_memory=False,)\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "executed-folks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 128,\n",
       " 16,\n",
       " [2, 2, 2, 2],\n",
       " [1, 1, 1, 1],\n",
       " [5, 5, 5, 5],\n",
       " [2, 2, 2, 2],\n",
       " [1, 16, 64, 64],\n",
       " [32, 64, 128, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals().update(config['model_params'])\n",
    "\n",
    "in_channels, latent_dim, depth_dim, xystrides, tstrides, kernels, mpkernels, input_size, hidden_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "speaking-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Encoder\n",
    "encoder = nn.ModuleList()\n",
    "for layer_n, h_dim in enumerate(hidden_dims):\n",
    "    encoder.add_module(str('ConvLSTM{}'.format(layer_n)),ConvLSTM(input_dim = in_channels,\n",
    "                                                                 hidden_dim = [h_dim],\n",
    "                                                                 kernel_size= (kernels[layer_n],kernels[layer_n]),\n",
    "                                                                 num_layers = 1,\n",
    "                                                                 batch_first=True,\n",
    "                                                                 ))\n",
    "\n",
    "    encoder.add_module(str('batchnorm%i' % layer_n), \n",
    "                                nn.BatchNorm3d(h_dim))\n",
    "    encoder.add_module(str('maxpool%i' % layer_n), \n",
    "                                nn.MaxPool2d(kernel_size=mpkernels[layer_n], \n",
    "                                stride=(xystrides[layer_n], xystrides[layer_n]), \n",
    "                                padding=0,return_indices=False))\n",
    "    encoder.add_module(str('relu%i' % layer_n), \n",
    "                                nn.LeakyReLU(0.05))\n",
    "    in_channels = h_dim\n",
    "fc_mu = nn.Linear(hidden_dims[-1]*depth_dim*4*4, latent_dim)\n",
    "fc_var = nn.Linear(hidden_dims[-1]*depth_dim*4*4, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "allied-orchestra",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM0 torch.Size([30, 16, 1, 64, 64])\n",
      "OutShape torch.Size([30, 32, 16, 64, 64])\n",
      "batchnorm0 torch.Size([30, 32, 16, 64, 64])\n",
      "OutShape torch.Size([30, 32, 16, 64, 64])\n",
      "maxpool0 torch.Size([30, 32, 16, 64, 64])\n",
      "OutShape torch.Size([30, 32, 16, 32, 32])\n",
      "relu0 torch.Size([30, 32, 16, 32, 32])\n",
      "OutShape torch.Size([30, 32, 16, 32, 32])\n",
      "ConvLSTM1 torch.Size([30, 32, 16, 32, 32])\n",
      "OutShape torch.Size([30, 64, 16, 32, 32])\n",
      "batchnorm1 torch.Size([30, 64, 16, 32, 32])\n",
      "OutShape torch.Size([30, 64, 16, 32, 32])\n",
      "maxpool1 torch.Size([30, 64, 16, 32, 32])\n",
      "OutShape torch.Size([30, 64, 16, 16, 16])\n",
      "relu1 torch.Size([30, 64, 16, 16, 16])\n",
      "OutShape torch.Size([30, 64, 16, 16, 16])\n",
      "ConvLSTM2 torch.Size([30, 64, 16, 16, 16])\n",
      "OutShape torch.Size([30, 128, 16, 16, 16])\n",
      "batchnorm2 torch.Size([30, 128, 16, 16, 16])\n",
      "OutShape torch.Size([30, 128, 16, 16, 16])\n",
      "maxpool2 torch.Size([30, 128, 16, 16, 16])\n",
      "OutShape torch.Size([30, 128, 16, 8, 8])\n",
      "relu2 torch.Size([30, 128, 16, 8, 8])\n",
      "OutShape torch.Size([30, 128, 16, 8, 8])\n",
      "ConvLSTM3 torch.Size([30, 128, 16, 8, 8])\n",
      "OutShape torch.Size([30, 256, 16, 8, 8])\n",
      "batchnorm3 torch.Size([30, 256, 16, 8, 8])\n",
      "OutShape torch.Size([30, 256, 16, 8, 8])\n",
      "maxpool3 torch.Size([30, 256, 16, 8, 8])\n",
      "OutShape torch.Size([30, 256, 16, 4, 4])\n",
      "relu3 torch.Size([30, 256, 16, 4, 4])\n",
      "OutShape torch.Size([30, 256, 16, 4, 4])\n",
      "mu: torch.Size([30, 128]) log_var torch.Size([30, 128])\n"
     ]
    }
   ],
   "source": [
    "x = batch\n",
    "B,T,C,H,W = x.shape\n",
    "for name, layer in encoder.named_children():\n",
    "    print(name,x.shape)\n",
    "    if isinstance(layer,ConvLSTM):\n",
    "        if x.shape[1] != T:\n",
    "            x = x.permute(0,2,1,3,4)\n",
    "        else:\n",
    "            pass\n",
    "        x, state = layer(x)\n",
    "        x = x[0].permute(0,2,1,3,4)\n",
    "    elif isinstance(layer,nn.MaxPool2d):\n",
    "        shape = x.shape\n",
    "        x = x.view(shape[0],shape[1]*shape[2],shape[3],shape[4])\n",
    "        x = layer(x)\n",
    "        x = x.view(shape[0],shape[1],shape[2],x.shape[-2],x.shape[-1])\n",
    "    else:\n",
    "        x = layer(x)\n",
    "    print('OutShape', x.shape)\n",
    "encoder_shapes = [x.shape]\n",
    "x = torch.flatten(x, start_dim=1)\n",
    "mu = fc_mu(x)\n",
    "log_var = fc_var(x)\n",
    "\n",
    "print('mu:',mu.shape,'log_var',log_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "colored-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    \"\"\"\n",
    "    Reparameterization trick to sample from N(mu, var) from\n",
    "    N(0,1).\n",
    "    :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "    :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "    :return: (Tensor) [B x D]\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps * std + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interior-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = reparameterize(mu, log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "genetic-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Decoder\n",
    "decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*encoder_shapes[-1][-1]*encoder_shapes[-1][-2]*encoder_shapes[-1][-3])\n",
    "\n",
    "hidden_dims.reverse()\n",
    "decoder = nn.ModuleList()\n",
    "for layer_n, i in enumerate(range(len(hidden_dims) - 1)):\n",
    "    decoder.add_module(str('ConvLSTM{}_transpose'.format(layer_n)), ConvLSTM(input_dim   = hidden_dims[i],\n",
    "                                                                             hidden_dim  = hidden_dims[i+1],\n",
    "                                                                             kernel_size = (kernels[layer_n],kernels[layer_n]),\n",
    "                                                                             num_layers  = 1,\n",
    "                                                                             batch_first = True,\n",
    "                                                                             use_transpose = True,\n",
    "                                                                         ))\n",
    "    decoder.add_module(str('upsample%i' % i),nn.Upsample(scale_factor=(1, xystrides[-1], xystrides[-1]), mode='nearest'))\n",
    "\n",
    "    decoder.add_module(str('batchnorm%i' % layer_n),\n",
    "                            nn.BatchNorm3d(hidden_dims[i + 1]))\n",
    "    decoder.add_module(str('relu%i' % layer_n), nn.LeakyReLU(0.05))\n",
    "\n",
    "\n",
    "final_layer = nn.ModuleList()\n",
    "final_layer.add_module(str('ConvLSTM{}_transpose'.format(layer_n)), ConvLSTM(input_dim   = hidden_dims[-1],\n",
    "                                                                         hidden_dim  = hidden_dims[-1],\n",
    "                                                                         kernel_size = (kernels[-1],kernels[-1]),\n",
    "                                                                         num_layers  = 1,\n",
    "                                                                         batch_first = True,\n",
    "                                                                         use_transpose = True,\n",
    "                                                                     ))\n",
    "final_layer.add_module(str('upsample%i' % i),nn.Upsample(scale_factor=(1, xystrides[-1], xystrides[-1]), mode='nearest'))\n",
    "\n",
    "final_layer.add_module(str('batchnorm%i' % layer_n), nn.BatchNorm3d(hidden_dims[-1]))\n",
    "final_layer.add_module(str('relu%i' % layer_n), nn.LeakyReLU(0.05))\n",
    "final_layer.add_module(str('last_conv%i' % 0), nn.Conv3d(hidden_dims[-1], out_channels=1,\n",
    "                                                             kernel_size= kernels[0], padding=2))\n",
    "final_layer.add_module(str('last_Tanh%i' % 0), nn.Tanh()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "opposed-folder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 16, 256, 4, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = decoder_input(z)\n",
    "\n",
    "result = result.view(-1,encoder_shapes[-1][1],encoder_shapes[-1][2],encoder_shapes[-1][-2],encoder_shapes[-1][-1])\n",
    "result = result.permute(0,2,1,3,4)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "separated-commercial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM0_transpose torch.Size([30, 16, 256, 4, 4])\n",
      "OutShape torch.Size([30, 128, 16, 4, 4])\n",
      "upsample0 torch.Size([30, 128, 16, 4, 4])\n",
      "OutShape torch.Size([30, 128, 16, 8, 8])\n",
      "batchnorm0 torch.Size([30, 128, 16, 8, 8])\n",
      "OutShape torch.Size([30, 128, 16, 8, 8])\n",
      "relu0 torch.Size([30, 128, 16, 8, 8])\n",
      "OutShape torch.Size([30, 128, 16, 8, 8])\n",
      "ConvLSTM1_transpose torch.Size([30, 128, 16, 8, 8])\n",
      "OutShape torch.Size([30, 64, 16, 8, 8])\n",
      "upsample1 torch.Size([30, 64, 16, 8, 8])\n",
      "OutShape torch.Size([30, 64, 16, 16, 16])\n",
      "batchnorm1 torch.Size([30, 64, 16, 16, 16])\n",
      "OutShape torch.Size([30, 64, 16, 16, 16])\n",
      "relu1 torch.Size([30, 64, 16, 16, 16])\n",
      "OutShape torch.Size([30, 64, 16, 16, 16])\n",
      "ConvLSTM2_transpose torch.Size([30, 64, 16, 16, 16])\n",
      "OutShape torch.Size([30, 32, 16, 16, 16])\n",
      "upsample2 torch.Size([30, 32, 16, 16, 16])\n",
      "OutShape torch.Size([30, 32, 16, 32, 32])\n",
      "batchnorm2 torch.Size([30, 32, 16, 32, 32])\n",
      "OutShape torch.Size([30, 32, 16, 32, 32])\n",
      "relu2 torch.Size([30, 32, 16, 32, 32])\n",
      "OutShape torch.Size([30, 32, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, layer in decoder.named_children():\n",
    "    print(name,result.shape)\n",
    "    if isinstance(layer,ConvLSTM):\n",
    "        if result.shape[1] != T:\n",
    "            result = result.permute(0,2,1,3,4)\n",
    "        else:\n",
    "            pass\n",
    "        result, state = layer(result)\n",
    "        result = result[0].permute(0,2,1,3,4)\n",
    "    else:\n",
    "        result = layer(result)\n",
    "    print('OutShape', result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deluxe-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM2_transpose torch.Size([30, 32, 16, 32, 32])\n",
      "OutShape torch.Size([30, 32, 16, 32, 32])\n",
      "upsample2 torch.Size([30, 32, 16, 32, 32])\n",
      "OutShape torch.Size([30, 32, 16, 64, 64])\n",
      "batchnorm2 torch.Size([30, 32, 16, 64, 64])\n",
      "OutShape torch.Size([30, 32, 16, 64, 64])\n",
      "relu2 torch.Size([30, 32, 16, 64, 64])\n",
      "OutShape torch.Size([30, 32, 16, 64, 64])\n",
      "last_conv0 torch.Size([30, 32, 16, 64, 64])\n",
      "OutShape torch.Size([30, 1, 16, 64, 64])\n",
      "last_Tanh0 torch.Size([30, 1, 16, 64, 64])\n",
      "OutShape torch.Size([30, 1, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for name, layer in final_layer.named_children():\n",
    "    print(name,result.shape)\n",
    "    if isinstance(layer,ConvLSTM):\n",
    "        if result.shape[1] != T:\n",
    "            result = result.permute(0,2,1,3,4)\n",
    "        else:\n",
    "            pass\n",
    "        result, state = layer(result)\n",
    "        result = result[0].permute(0,2,1,3,4)\n",
    "    else:\n",
    "        result = layer(result)\n",
    "    print('OutShape', result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "noted-street",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1, 16, 64, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-weekly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:VAE]",
   "language": "python",
   "name": "conda-env-VAE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
